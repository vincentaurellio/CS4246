{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f833d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_env_copy import GameEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d61741",
   "metadata": {},
   "source": [
    "### Control Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2c268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv((20,20), (3,3), (18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c37b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, bias_const=0.0):\n",
    "    \"\"\"Initialize the weights and biases of a layer.\n",
    "\n",
    "    Args:\n",
    "        layer (nn.Module): The layer to initialize.\n",
    "        std (float): Standard deviation for orthogonal initialization.\n",
    "        bias_const (float): Constant value for bias initialization.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The initialized layer.\n",
    "    \"\"\"\n",
    "    torch.nn.init.xavier_uniform_(layer.weight)  # Orthogonal initialization\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)  # Constant bias\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879efc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, actor_input_size, critic_input_size, actor_output_size, critic_output_size):\n",
    "        super(Agent, self).__init__()\n",
    "        hidden_size1, hidden_size2, hidden_size3 = 64, 128, 64\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(actor_input_size, hidden_size1)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size1, hidden_size2)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size2, hidden_size3)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size3, actor_output_size))\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(critic_input_size, hidden_size1)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size1, hidden_size2)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size2, hidden_size3)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Linear(hidden_size3, critic_output_size))\n",
    "        )\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return self.critic(state)\n",
    "\n",
    "    def get_action_probs(self, state):\n",
    "        return torch.distributions.categorical.Categorical(logits = self.actor(state))\n",
    "\n",
    "    def get_action(self, probs):\n",
    "        return probs.sample()\n",
    "\n",
    "    def get_action_logprob(self, probs, action):\n",
    "        return probs.log_prob(action)\n",
    "\n",
    "    def get_entropy(self, probs):\n",
    "        return probs.entropy()\n",
    "\n",
    "    def get_action_logprob_entropy(self, state):\n",
    "        probs = self.get_action_probs(state)\n",
    "        action = self.get_action(probs)\n",
    "        logprob = self.get_action_logprob(probs, action)\n",
    "        entropy = self.get_entropy(probs)\n",
    "        return action, logprob, entropy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent_final = Agent(404, 404, 4, 1).to(device)\n",
    "agent_adv = Agent(404, 404, 4, 1).to(device)\n",
    "agent_trunc = Agent(404, 404, 4, 1).to(device)\n",
    "agent_less = Agent(404, 404, 4, 1).to(device)\n",
    "agent_op = Agent(404, 404, 4, 1).to(device)\n",
    "\n",
    "agent_final.load_state_dict(torch.load('model_final.pth', map_location=device))\n",
    "agent_adv.load_state_dict(torch.load('model_adversary.pth', map_location=device))\n",
    "agent_trunc.load_state_dict(torch.load('model_trunc.pth', map_location=device))\n",
    "agent_less.load_state_dict(torch.load('model_less.pth', map_location=device))\n",
    "agent_op.load_state_dict(torch.load('model_op.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent_final, agent_adv, agent_trunc, agent_less, agent_op]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env=GameEnv((20,20), (3,3), (18,18)), num_eps=10):\n",
    "    rewards = []\n",
    "    goal_reached = np.zeros(num_eps)\n",
    "\n",
    "    for episode in tqdm.tqdm(range(num_eps)):\n",
    "        total_reward = 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        for i in range(5000):\n",
    "            state_tensor = torch.tensor(state,dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = agent.get_action_logprob_entropy(state_tensor)\n",
    "\n",
    "            next_state, reward, done, _, infos = env.step(action.cpu().item())\n",
    "            total_reward += reward\n",
    "\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            \n",
    "    #         print(state[-4:])\n",
    "            \n",
    "            if done:\n",
    "                goal_reached[episode] = 1\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\nMean Rewards: {np.mean(rewards)}{', never reach goal' if not goal_reached.any() else ''}\\n\")\n",
    "    return np.mean(rewards), goal_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\Vincent\\AppData\\Local\\Temp\\ipykernel_55804\\1894394970.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state,dtype=torch.float32).to(device)\n",
      "100%|██████████| 10/10 [01:56<00:00, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Rewards: -22508.1, never reach goal\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "adv_results, adv_goals = test(agent_op, env = GameEnv((20,20), (3,3), (np.random.randint(12, 21),np.random.randint(12,21))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agents, *args):\n",
    "    rewards = []\n",
    "    goal_reached = []\n",
    "    for agent in agents:\n",
    "        reward, goal_reach = test(agent, *args)\n",
    "        rewards.append(reward)\n",
    "        goal_reached.append(goal_reach)\n",
    "    return rewards, goal_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 7),np.random.randint(1,7)), (18, 18)), 1) for i in range(10) ]\n",
    "results4 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 21),np.random.randint(1,21)), (np.random.randint(1, 21),np.random.randint(1,21))), 1) for i in range(10) ]\n",
    "results2= [test_agents(agents, GameEnv((20,20), (3,3), (np.random.randint(12, 21),np.random.randint(12,21))), 1) for i in range(10)]\n",
    "results1= test_agents(agents, GameEnv((20,20), (3,3), (18, 18)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 7),np.random.randint(1,7)), (np.random.randint(12, 21),np.random.randint(12,21))), 1) for i in range(10) ]\n",
    "results5 = test_agents(agents, GameEnv((20, 20), (18, 18), (3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-75788. , -35158.4, -65468.2, -88416. , -33006.5]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reward_goal(result):\n",
    "    reward = []\n",
    "    goal = []\n",
    "    for res in result:\n",
    "        reward.append(res[0])\n",
    "        goal.append(res[1])\n",
    "    return np.mean(np.array(reward), axis=0), np.hstack(np.array(goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bfbda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-94686.8, -86408.8, -47670.1, -95161.7,   1553.3]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(map(np.array, results1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0144605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-93635.2, -21478.2, -64243.3, -96585.2,  -4737. ]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 1.]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac54218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-93186.3, -37477.5, -78592. , -95165.9,   1499.4]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-75788. , -35158.4, -65468.2, -88416. , -33006.5]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = test_agents(agents, GameEnv((20,20), (3,3), (18, 18)), 10) # training env\n",
    "results2 = test_agents(agents, GameEnv((20, 20), (18, 18), (3, 3)), 10) # swapped start goal\n",
    "results3 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 7),np.random.randint(1,7)), (np.random.randint(12, 21),np.random.randint(12,21))), 1) for i in range(10) ] # up right\n",
    "results4 = [test_agents(agents, GameEnv((20,20), (np.random.randint(12, 21),np.random.randint(12,21)), (np.random.randint(1, 7),np.random.randint(1,7))), 1) for i in range(10) ] # down left\n",
    "results5 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 7),np.random.randint(12,21)), (np.random.randint(12, 21),np.random.randint(1,7))), 1) for i in range(10) ] # down right\n",
    "results6 = [test_agents(agents, GameEnv((20,20), (np.random.randint(12, 21),np.random.randint(1,7)), (np.random.randint(1, 7),np.random.randint(12,21))), 1) for i in range(10) ] # up left\n",
    "results7 = [test_agents(agents, GameEnv((20,20), (np.random.randint(1, 21),np.random.randint(1,21)), (np.random.randint(1, 21),np.random.randint(1,21))), 1) for i in range(10) ] # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-94354.7, -75600.0, -59692.4, -95139.0, 1552.2],\n",
       " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-32440.4, -9206.1, -26816.8, -95151.1, 1464.4],\n",
       " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-78536.8, -27256.4, -67310.1, -81137.2, -22142. ]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -51251. ,  -54823.4,  -41434. , -100502.9,   -9650.8]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-145533.9,  -52801.8,  -30474.4, -157471.8,  -76985.1]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -13841.7,  -61576.2, -145610. ,  -22170.5,  -75503.5]),\n",
       " array([[0., 0., 0., 0., 0., 1., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-82185.1, -30136.7, -62310.3, -93336.8, -34681.1]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_goal(results7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
